---
title: "Lab1_Classification"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Lab 1_ Classification

#Method 1: Tree based classification

step 1: collecting data 

```{r }
getwd()
setwd('C:/Users/Madhu/Side Projects/Machine_Learning_Course')

#Read data file

credit <- read.csv('credit.csv')

str(credit)

```

Step 2: Exploring the data


```{r }
#Using sumary to check the statistics of amunt column in the dataset

summary(credit$amount)

str(credit$default)

#Default has two levels - yes or no
#Using table to see how many were defaulted and how many were not

table(credit$default)

```




Steps to develop tree based classification


```{r }
#Before creating the testing and training data, randomizing the observations

set.seed(12345)

credit_rand <- credit[order(runif(1000)),]

#checking the summary of the original data with the randomized one to notice any substantial changes 

summary(credit$amount)
summary(credit_rand$amount)

```



Splitting data to training and testing set 


```{r }
# Choosing 90% for training set and remaining 10% as the testing set

credit_train <- credit_rand[1:900,]
credit_test <- credit_rand[901:1000,]

#Looking at percentage split of the testing and training to see if randomization went well

prop.table(table(credit_train$default))
prop.table(table(credit_train$default))

```


Training a model on the data


```{r }

#install.packages('C50')
library(C50)
# Buiding ecision tree. Since we are predicting defaulted or not, we must specify the 17th column to represent it as the class or response variable 

credit_model <- C5.0(x = credit_train[-17], y = credit_train$default)
credit_model

```
Examining the decision tree


```{r }

summary(credit_model)

```

As we can see, 590 were classified as class a, 166 were classified as class b . There were 125 misclassified.


#step 4: Evaluating Model performance

```{r }

cred_pred <- predict(credit_model, credit_test)
# using gmodels ro create confusion matrix

#install.packages('gmodels')

library(gmodels)

CrossTable(credit_test$default, cred_pred, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, dnn = c('actual default', 'predicted default'))

```

As seen from the above confusion matrix, 11 were misclassified as a Type 2 error and 16 were type 1 that is it was yes but classifed as no.


##Method#2: Adding Regression to trees

#Step 1: Collecting data 

```{r }
# Read data 
wine <- read.csv("whitewines.csv")
str(wine)

```


```{r }
# Checkimg the distribution of quality variable to see if its normal 
hist(wine$quality)

```


Yes, the distribution seems pretty normal and we can use regression on this class variable. 

Next, we explore and prepare data in step 2.

```{r }
# Creating training and testing data set 
# 75% for tarining and 25% for testing

wine_train <- wine[1:3750, ]
wine_test <- wine[3751:4898, ]


```

Step 3, training model on data 


```{r }
# Installing rpart - recursive partitioning 

#install.packages('rpart')
library(rpart)

m.rpart <- rpart(quality ~ ., data=wine_train)
m.rpart

#install.packages('rpart.plot')

library(rpart.plot)

#Visualizing the tree

rpart.plot(m.rpart, digits=3)

rpart.plot(m.rpart, digits=4, fallen.leaves = TRUE, type = 3, extra = 101)

```

The last and final step 4 of evaluating model performance

```{r }

p.rpart <- predict(m.rpart, wine_test)
summary(p.rpart)
summary(wine_test$quality)
cor(p.rpart, wine_test$quality)

```

A 54% correlation is seen. 

